{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Tarefa 02: RAG Multi-modal\n","\n","* Professor Respons√°vel:  Julio Cesar dos Reis <a href=\"mailto:dosreis@unicamp.br\">(dosreis@unicamp.br)</a>\n","\n","* Professores Assistentes: Fillipe Santos <a href=\"mailto:fillipesantos00@gmail.com\">(fillipesantos00@gmail.com)</a> e Seyed Jamal Haddadi <a href=\"mailto:s.j.haddadi88@gmail.com\">(s.j.haddadi88@gmail.com)</a>\n","\n","\n"],"metadata":{"id":"QgElAtwQ9VtI"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"KmvKFg3JpaO8","executionInfo":{"status":"error","timestamp":1739230043272,"user_tz":180,"elapsed":725,"user":{"displayName":"Seyed Jamal Haddadi","userId":"01298913454198494240"}},"colab":{"base_uri":"https://localhost:8080/","height":383},"outputId":"6db82dc7-cac3-44df-e56d-b101dcc80070"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'functions'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-fb0d0b52d245>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCLIPProcessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIPModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'functions'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["from bs4 import BeautifulSoup\n","import os\n","from functions import *\n","from PIL import Image\n","from transformers import CLIPProcessor, CLIPModel\n","from torch import cat, save"]},{"cell_type":"code","source":["import os\n","\n","# Get a list of all HTML files in the 'raw' directory\n","\n","\n","# Initialize lists to store extracted text and images\n","\n","\n","# Process each HTML file\n"],"metadata":{"id":"i2bFtdLuplZN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_list = [\n","    content['section'] + \": \" + content['text'][:256 - len(content['section']) - 2]\n","    for content in text_content_list\n","]\n","\n","# # This line creates a list of image objects by opening each image file specified in 'image_path' from the 'image_content_list'.\n"],"metadata":{"id":"qxPCTQ8ttGOv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the pre-trained CLIP model (openai/clip-vit-base-patch16) and processor (openai/clip-vit-base-patch16) from the OpenAI repository to handle both image and text inputs.\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3dPOV4OztMAh","executionInfo":{"status":"ok","timestamp":1738336658220,"user_tz":180,"elapsed":2296,"user":{"displayName":"Soheil Haddadi","userId":"07947715605606882275"}},"outputId":"9e64dcfc-56d8-43fe-b393-a3bb364c27af"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# This line processes the input text and images, converts them into tensors (for model input),\n","# and ensures proper padding to handle variable sequence lengths.\n"],"metadata":{"id":"IGpo9XTPvcC1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["outputs = model(**inputs)"],"metadata":{"id":"dPyKkTXcvgzu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extract the text and image embeddings from the output for further processing or analysis\n","\n"],"metadata":{"id":"kkBpmSxNvpII"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["save_to_json(text_content_list, output_file='data/text_content.json')\n","save_to_json(image_content_list, output_file='data/image_content.json')"],"metadata":{"id":"XS3DYSXpvyv1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["save(text_embeddings, 'data/text_embeddings.pt')\n","save(image_embeddings, 'data/image_embeddings.pt')"],"metadata":{"id":"weLeZKvdwEKx"},"execution_count":null,"outputs":[]}]}