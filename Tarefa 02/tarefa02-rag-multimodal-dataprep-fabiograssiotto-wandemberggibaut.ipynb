{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CURSO INF0083 - TECNOLOGIAS AVANÇADAS EM IA  \n",
    "##### DISCIPLINA INF0084 - Sistemas Inteligentes e Técnicas Avançadas em IA  \n",
    "##### Tarefa 02: RAG Multimodal\n",
    "##### Alunos: Fabio Grassiotto e Wandemberg Gibaut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import glob\n",
    "import requests\n",
    "import unicodedata\n",
    "import json\n",
    "from functions import *\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torch import cat, save\n",
    "from io import BytesIO\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leitura dos arquivos html e criação das listas de textos e imagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de arquivos no diretórion \"raw\"\n",
    "html_files = glob.glob(os.path.join('raw', '*.html'))\n",
    "\n",
    "texts = []\n",
    "images = []\n",
    "images_list = []\n",
    "\n",
    "def download_image(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        return img\n",
    "    else:\n",
    "        print(f\"Failed to download {url}\")\n",
    "        return None\n",
    "\n",
    "# Remoção de caracteres com Unicode\n",
    "def normalize_text(text):\n",
    "    return unicodedata.normalize('NFKD', text)\n",
    "    \n",
    "# Popula listas\n",
    "for file in html_files:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        soup = BeautifulSoup(f, 'html.parser')\n",
    "        \n",
    "        # Extração do texto\n",
    "        text = soup.get_text()\n",
    "        text = normalize_text(text)\n",
    "        text_lines = text.split('\\n')\n",
    "        texts.extend([line.strip() for line in text_lines if line.strip()])        \n",
    "        \n",
    "        # Criação de objetos para imagens\n",
    "        for img in soup.find_all('img'):\n",
    "            img_src = img.get('src')\n",
    "            images_list.append(img_src)\n",
    "            pil_image = download_image(img_src)\n",
    "            if pil_image:\n",
    "                images.append(pil_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspeção dos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Multimodal Models — LLMs that can see and hear',\n",
       " 'Multimodal Models — LLMs that can see and hear',\n",
       " 'An introduction with example Python code',\n",
       " 'Multimodal Models — LLMs That Can See and HearAn introduction with example Python codeThis is the first post in a larger series on Multimodal AI. A Multimodal Model (MM) is an AI system capable of processing or generating multiple data modalities (e.g., text, image, audio, video). In this article, I will discuss a particular type of MM that builds on top of a large language model (LLM). I’ll start with a high-level overview of such models and then share example code for using LLaMA 3.2 Vision to perform various image-to-text tasks.Photo by Sincerely Media on UnsplashLarge language models (LLMs) have marked a fundamental shift in AI research and development. However, despite their broader impacts, they are still fundamentally limited.Namely, LLMs can only process and generate text, making them blind to other modalities such as images, video, audio, and more. This is a major limitation since some tasks rely on non-text data, e.g., analyzing engineering blueprints, reading body language or speech tonality, and interpreting plots and infographics.This has sparked efforts toward expanding LLM functionality to include multiple modalities.What is a Multimodal Model?A Multimodal Model (MM) is an AI system that can process multiple data modalities as input or output (or both) [1]. Below are a few examples.GPT-4o — Input: text, images, and audio. Output: text.FLUX — Input: text. Output: images.Suno — Input: text. Output: audio.Example mutlimodal models. Image by author.While there are several ways to create models that can process multiple data modalities, a recent line of research seeks to use LLMs as the core reasoning engine of a multimodal system [2]. Such models are called multimodal large language models (or large multimodal models) [2][3].One benefit of using existing LLM as a starting point for MMs is that they’ve demonstrated a strong ability to acquire world knowledge through large-scale pre-training, which can be leveraged to process concepts appearing in non-textual representations.3 Paths to MultimodalityHere, I will focus on multimodal models developed from an LLM. Three popular approaches are described below.LLM + Tools: Augment LLMs with pre-built componentsLLM + Adapters: Augment LLMs with multi-modal encoders or decoders, which are aligned via adapter fine-tuningUnified Models: Expand LLM architecture to fuse modalities at pre-trainingPath 1: LLM + ToolsThe simplest way to make an LLM multimodal is by adding external modules that can readily translate between text and an arbitrary modality. For example, a transcription model (e.g. Whisper) can be connected to an LLM to translate input speech into text, or a text-to-image model can generate images based on LLM outputs.The key benefit of such an approach is simplicity. Tools can quickly be assembled without any additional model training.The downside, however, is that the quality of such a system may be limited. Just like when playing a game of telephone, messages mutate when passed from person to person. Information may degrade going from one module to another via text descriptions only.An example of information degradation during message passing. Image by author.Path 2: LLM + AdaptersOne way to mitigate the “telephone problem” is by optimizing the representations of new modalities to align with the LLM’s internal concept space. For example, ensuring an image of a dog and the description of one look similar to the LLM.This is possible through the use of adapters, a relatively small set of parameters that appropriately translate a dense vector representation for a downstream model [2][4][5].Adapters can be trained using, for example, image-caption pairs, where the adapter learns to translate an image encoding into a representation compatible with the LLM [2][4][6]. One way to achieve this is via contrastive learning [2], which I will discuss more in the next article of this series.A simple strategy for integrating images into an LLM via an image encoding adapter. Image by author.The benefits of using adapters to augment LLMs include better alignment between novel modality representations in a data-efficient way. Since many pre-trained embedding, language, and diffusion models are available in today’s AI landscape, one can readily fuse models based on their needs. Notable examples from the open-source community are LLaVA, LLaMA 3.2 Vision, Flamingo, MiniGPT4, Janus, Mini-Omni2, and IDEFICS [3][5][7][8].However, this data efficiency comes at a price. Just like how adapter-based fine-tuning approaches (e.g. LoRA) can only nudge an LLM so far, the same holds in this context. Additionally, pasting various encoders and decoders to an LLM may result in overly complicated model architectures.Path 3: Unified ModelsThe final way to make an LLM multimodal is by incorporating multiple modalities at the pre-training stage. This works by adding modality-specific tokenizers (rather than pre-trained encoder/decoder models) to the model architecture and expanding the embedding layer to accommodate new modalities [9].While this approach comes with significantly greater technical challenges and computational requirements, it enables the seamless integration of multiple modalities into a shared concept space, unlocking better reasoning capabilities and efficiencies [10].The preeminent example of this unified approach is (presumably) GPT-4o, which processes text, image, and audio inputs to enable expanded reasoning capabilities at faster inference times than previous versions of GPT-4. Other models that follow this approach include Gemini, Emu3, BLIP, and Chameleon [9][10].Training these models typically entails multi-step pre-training on a set of (multimodal) tasks, such as language modeling, text-image contrastive learning, text-to-video generation, and others [7][9][10].Example: Using LLaMA 3.2 Vision for Image-based TasksWith a basic understanding of how LLM-based multimodal models work under the hood, let’s see what we can do with them. Here, I will use LLaMA 3.2 Vision to perform various image-to-text tasks.To run this example, download Ollama and its Python library. This enables the model to run locally i.e. no need for external API calls.The example code is freely available on GitHub.Importing modelWe start by importing ollama.import ollamaNext, we’ll download the model locally. Here, we use LLaMA 3.2 Vision 11B.ollama.pull(\\'llama3.2-vision\\')Visual QANow, we’re ready to use the model! Here’s how we can do basic visual question answering.# pass image and question to modelresponse = ollama.chat(    model=\\'llama3.2-vision\\',    messages=[{        \\'role\\': \\'user\\',        \\'content\\': \\'What is in this image?\\',        \\'images\\': [\\'images/shaw-sitting.jpeg\\']    }])# print responseprint(response[\\'message\\'][\\'content\\'])The image is of me from a networking event (as shown below).Image of me from networking event at Richardson IQ. Image by author.The model’s response is shown below. While it has trouble reading what’s on my hat, it does a decent job inferring the context of the photo.This image shows a man sitting on a yellow ottoman with his hands clasped together. He is wearing a black polo shirt with a name tag that says \"Shaw\" and a black baseball cap with white text that reads, \"THE DATA ENREPRENEUR.\" The background of the image appears to be an office or lounge area, with a large screen on the wall behind him displaying a presentation slide. There are also several chairs and tables in the background, suggesting that this may be a meeting room or common area for employees to gather and work.If you run this on your machine, you may run into a long wait time until the model generates a response. One thing we can do to make this less painful is to enable streaming.# create streamstream = ollama.chat(    model=\\'llama3.2-vision\\',    messages=[{        \\'role\\': \\'user\\',        \\'content\\': \\'Can you write a caption for this image?\\',        \\'images\\': [\\'images/shaw-sitting.jpeg\\']    }],    stream=True,)# print chunks in stream as they become availablefor chunk in stream:    print(chunk[\\'message\\'][\\'content\\'], end=\\'\\', flush=True)Interestingly, we get a qualitatively different response when prompting the model in a slightly different way for the same image.This image features a man sitting on a yellow chair. He is wearing a black polo shirt with a blue name tag that says \"Shaw\", khaki pants, and a black baseball cap with white text that reads \"THE DATA ENTHUSIAST\". The man has his hands clasped together in front of him and appears to be smiling.The background of the image consists of a room with various pieces of furniture. There is a green ottoman to the left of the yellow chair, and two blue chairs on the right side of the image. A brown table or desk sits behind the man, along with a fireplace. The walls are painted teal blue and have a wooden accent wall featuring holes for hanging items.The overall atmosphere suggests that this may be a modern office space or co-working area where people can come to work, relax, or socialize.Explaining MemesObjectively describing a scene is simpler than understanding and explaining humor. Let’s see how the model explains the meme below.Building with AI meme. Image by author.# ask model to explain memestream = ollama.chat(    model=\\'llama3.2-vision\\',    messages=[{        \\'role\\': \\'user\\',        \\'content\\': \\'Can you explain this meme to me?\\',        \\'images\\': [\\'images/ai-meme.jpeg\\']    }],    stream=True,)# print streamfor chunk in stream:    print(chunk[\\'message\\'][\\'content\\'], end=\\'\\', flush=True)The meme depicts Patrick Star from SpongeBob SquarePants, surrounded by various AI tools and symbols. The caption reads \"Trying to build with AI today...\" The image humorously illustrates the challenges of using AI in building projects, implying that it can be overwhelming and frustrating.The model does a good job here. It understands that the image is funny while also conveying the pain that people face.OCRThe last use case is optical character recognition (OCR). This involves extracting text from images, which is valuable in a wide range of contexts. Here, I’ll see if the model can translate a screenshot from my notes app to a markdown file.Screenshot of 5 AI project ideas. Image by author.# ask model to read screenshot and convert to markdownstream = ollama.chat(    model=\\'llama3.2-vision\\',    messages=[{        \\'role\\': \\'user\\',        \\'content\\': \\'Can you transcribe the text from this screenshot in a \\\\                    markdown format?\\',        \\'images\\': [\\'images/5-ai-projects.jpeg\\']    }],    stream=True,)# read streamfor chunk in stream:    print(chunk[\\'message\\'][\\'content\\'], end=\\'\\', flush=True)Here is the transcription of the text in markdown format:5 AI Projects You Can Build This Weekend (with Python)1. **Resume Optimization (Beginner)** * Idea: build a tool that adapts your resume for a specific job description2. **YouTube Lecture Summarizer (Beginner)** * Idea: build a tool that takes YouTube video links and summarizes them3. **Automatically Organizing PDFs (Intermediate)** * Idea: build a tool to analyze the contents of each PDF and organize them into folders based on topics4. **Multimodal Search (Intermediate)** * Idea: use multimodal embeddings to represent user queries, text knowledge, and images in a single space5. **Desktop QA (Advanced)** * Idea: connect a multimodal knowledge base to a multimodal model like Llama-3.2-11B-VisionNote that I\\'ve added some minor formatting changes to make the text more readable in markdown format. Let me know if you have any further requests.Again, the model does a decent job out of the box. While it missed the header, it accurately captured the content and formatting of the project ideas.YouTube-Blog/multimodal-ai/1-mm-llms at main · ShawhinT/YouTube-BlogCodes to complement YouTube videos and blog posts on Medium. - YouTube-Blog/multimodal-ai/1-mm-llms at main ·...github.comWhat’s next?Multimodal models are AI systems that can process multiple data modalities as inputs or outputs (or both). A recent trend for developing these systems involves adding modalities to large language models (LLMs).However, there are other types of multimodal models. In the next article of this series, I will discuss multimodal embedding models, which encode multiple data modalities (e.g. text and images) into a shared representation space.More on Multimodal models 👇Multimodal AIshawhin.medium.com👉 Get FREE access to every new story I write (Learn More)[1] Multimodal Machine Learning: A Survey and Taxonomy[2] A Survey on Multimodal Large Language Models[3] Visual Instruction Tuning[4] GPT-4o System Card[5] Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation[6] Learning Transferable Visual Models From Natural Language Supervision[7] Flamingo: a Visual Language Model for Few-Shot Learning[8] Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex Capabilities[9] Emu3: Next-Token Prediction is All You Need[10] Chameleon: Mixed-Modal Early-Fusion Foundation Models',\n",
       " 'By Shaw Talebi on November 19, 2024.Canonical linkExported from Medium on December 2, 2024.',\n",
       " 'Multimodal Embeddings: An Introduction',\n",
       " 'Multimodal Embeddings: An Introduction',\n",
       " 'Mapping text and images into a common space',\n",
       " 'Multimodal Embeddings: An IntroductionMapping text and images into a common spaceThis is the 2nd article in a larger series on multimodal AI. In the previous post, we saw how to augment large language models (LLMs) to understand new data modalities (e.g., images, audio, video). One such approach relied on encoders that generate vector representations (i.e. embeddings) of non-text data. In this article, I will discuss multimodal embeddings and share what they can do via two practical use cases.Image from Canva.AI research is traditionally split into distinct fields: NLP, computer vision (CV), robotics, human-computer interface (HCI), etc. However, countless practical tasks require the integration of these different research areas e.g. autonomous vehicles (CV + robotics), AI agents (NLP + CV + HCI), personalized learning (NLP + HCI), etc.Although these fields aim to solve different problems and work with different data types, they all share a fundamental process. Namely, generating useful numerical representations of real-world phenomena.Historically, this was done by hand. This means that researchers and practitioners would use their (or other people’s) expertise to explicitly transform data into a more helpful form. Today, however, these can be derived another way.EmbeddingsEmbeddings are (useful) numerical representations of data learned implicitly through model training. For example, through learning how to predict text, BERT learned representations of text, which are helpful for many NLP tasks [1]. Another example is the Vision Transformer (ViT), trained for image classification on Image Net, which can be repurposed for other applications [2].A key point here is that these learned embedding spaces will have some underlying structure so that similar concepts are located close together. As shown in the toy examples below.Toy represetation of text and image embeddings, respectively. Image by author.One key limitation of the previously mentioned models is they are restricted to a single data modality, e.g., text or images. Preventing cross-modal applications like image captioning, content moderation, image search, and more. But what if we could merge these two representations?Multimodal EmbeddingsAlthough text and images may look very different to us, in a neural network, these are represented via the same mathematical object, i.e., a vector. Therefore, in principle, text, images, or any other data modality can processed by a single model.This fact underlies multimodal embeddings, which represent multiple data modalities in the same vector space such that similar concepts are co-located (independent of their original representations).Toy representation of multimodal embedding space. Image by author.For example, CLIP encodes text and images into a shared embedding space [3]. A key insight from CLIP is that by aligning text and image representations, the model is capable of 0-shot image classification on an arbitrary set of target classes since any input text can be treated as a class label (we will see a concrete example of this later).However, this idea is not limited to text and images. Virtually any data modalities can be aligned in this way e.g., text-audio, audio-image, text-EEG, image-tabular, and text-video. Unlocking use cases such as video captioning, advanced OCR, audio transcription, video search, and EEG-to-text [4].Contrastive LearningThe standard approach to aligning disparate embedding spaces is contrastive learning (CL). A key intuition of CL is to represent different views of the same information similarly [5].This consists of learning representations that maximize the similarity between positive pairs and minimize the similarity of negative pairs. In the case of an image-text model, a positive pair might be an image with an appropriate caption, while a negative pair would be an image with an irrelevant caption (as shown below).Example positive and negative pairs used in contrastive training. Image by author.Two key aspects of CL contribute to its effectivenessSince positive and negative pairs can be curated from the data’s inherent structure (e.g., metadata from web images), CL training data do not require manual labeling, which unlocks larger-scale training and more powerful representations [3].It simultaneously maximizes positive and minimizes negative pair similarity via a special loss function, as demonstrated by CLIP [3].CLIP’s contrastive loss for text-image representation alignment [3]. Image by author.Example Code: Using CLIP for 0-shot classification and image searchWith a high-level understanding of how multimodal embeddings work, let’s see two concrete examples of what they can do. Here, I will use the open-source CLIP model to perform two tasks: 0-shot image classification and image search.The code for these examples is freely available on the GitHub repository.Use case 1: 0-shot Image ClassificationThe basic idea behind using CLIP for 0-shot image classification is to pass an image into the model along with a set of possible class labels. Then, a classification can be made by evaluating which text input is most similar to the input image.We’ll start by importing the Hugging Face Transformers library so that the CLIP model can be downloaded locally. Additionally, the PIL library is used to load images in Python.from transformers import CLIPProcessor, CLIPModelfrom PIL import ImageNext, we can import a version of the clip model and its associated data processor. Note: the processor handles tokenizing input text and image preparation.# import modelmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")# import processor (handles text tokenization and image preprocessing)processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\") We load in the below image of a cat and create a list of two possible class labels: “a photo of a cat” or “a photo of a dog”.# load imageimage = Image.open(\"images/cat_cute.png\")# define text classestext_classes = [\"a photo of a cat\", \"a photo of a dog\"]Input cat photo. Image from Canva.Next, we’ll preprocess the image/text inputs and pass them into the model.# pass image and text classes to processorinputs = processor(text=text_classes, images=image, return_tensors=\"pt\",                                                     padding=True)# pass inputs to CLIPoutputs = model(**inputs) # note: \"**\" unpacks dictionary itemsTo make a class prediction, we must extract the image logits and evaluate which class corresponds to the maximum.# image-text similarity scorelogits_per_image = outputs.logits_per_image # convert scores to probs via softmaxprobs = logits_per_image.softmax(dim=1) # print predictionpredicted_class = text_classes[probs.argmax()]print(predicted_class, \"| Probability = \",                        round(float(probs[0][probs.argmax()]),4))>> a photo of a cat | Probability =  0.9979The model nailed it with a 99.79% probability that it’s a cat photo. However, this was a super easy one. Let’s see what happens when we change the class labels to: “ugly cat” and “cute cat” for the same image.>> cute cat | Probability =  0.9703The model easily identified that the image was indeed a cute cat. Let’s do something more challenging like the labels: “cat meme” or “not cat meme”.>> not cat meme | Probability =  0.5464While the model is less confident about this prediction with a 54.64% probability, it correctly implies that the image is not a meme.Use case 2: Image SearchAnother application of CLIP is essentially the inverse of Use Case 1. Rather than identifying which text label matches an input image, we can evaluate which image (in a set) best matches a text input (i.e. query)—in other words, performing a search over images.We start by storing a set of images in a list. Here, I have three images of a cat, dog, and goat, respectively.# create list of images to search overimage_name_list = [\"images/cat_cute.png\", \"images/dog.png\", \"images/goat.png\"]image_list = []for image_name in image_name_list:    image_list.append(Image.open(image_name))Next, we can define a query like “a cute dog” and pass it and the images into CLIP.# define a queryquery = \"a cute dog\"# pass images and query to CLIPinputs = processor(text=query, images=image_list, return_tensors=\"pt\",                                                   padding=True)We can then match the best image to the input text by extracting the text logits and evaluating the image corresponding to the maximum.# compute logits and probabilitiesoutputs = model(**inputs)logits_per_text = outputs.logits_per_textprobs = logits_per_text.softmax(dim=1)# print best matchbest_match = image_list[probs.argmax()]prob_match = round(float(probs[0][probs.argmax()]),4)print(\"Match probability: \",prob_match)display(best_match)>> Match probability:  0.9817Best match for query “a cute dog”. Image from Canva.We see that (again) the model nailed this simple example. But let’s try some trickier examples.query = \"something cute but metal 🤘\">> Match probability:  0.7715Best match for query “something cute but metal 🤘”. Image from Canva.query = \"a good boy\">> Match probability:  0.8248Best match for query “a good boy”. Image from Canva.query = \"the best pet in the world\">> Match probability:  0.5664Best match for query “the best pet in the world”. Image from Canva.Although this last prediction is quite controversial, all the other matches were spot on! This is likely since images like these are ubiquitous on the internet and thus were seen many times in CLIP’s pre-training.YouTube-Blog/multimodal-ai/2-mm-embeddings at main · ShawhinT/YouTube-BlogCodes to complement YouTube videos and blog posts on Medium. - YouTube-Blog/multimodal-ai/2-mm-embeddings at main ·...github.comWhat’s Next?Multimodal embeddings unlock countless AI use cases that involve multiple data modalities. Here, we saw two such use cases, i.e., 0-shot image classification and image search using CLIP.Another practical application of models like CLIP is multimodal RAG, which consists of the automated retrieval of multimodal context to an LLM. In the next article of this series, we will see how this works under the hood and review a concrete example.More on Multimodal models 👇Multimodal AIEdit descriptionshawhin.medium.comMy website: https://www.shawhintalebi.com/[1] BERT[2] ViT[3] CLIP[4] Thought2Text: Text Generation from EEG Signal using Large Language Models (LLMs)[5] A Simple Framework for Contrastive Learning of Visual Representations',\n",
       " 'By Shaw Talebi on November 29, 2024.Canonical linkExported from Medium on December 2, 2024.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=800x600>,\n",
       " <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=800x262>,\n",
       " <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=800x322>,\n",
       " <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=800x378>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=800x1016>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=800x800>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=800x1308>,\n",
       " <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=800x450>,\n",
       " <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=800x439>,\n",
       " <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=800x586>,\n",
       " <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=800x435>,\n",
       " <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=800x423>,\n",
       " <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=366x342>,\n",
       " <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=366x273>,\n",
       " <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=288x256>,\n",
       " <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=366x273>,\n",
       " <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=366x342>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carrega o modelo pré-treinado do CLIP e o processador do repositório da OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"openai/clip-vit-base-patch16\"\n",
    "model = CLIPModel.from_pretrained(model_name)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geração dos embeddings para os textos e imagens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_inputs = processor(text=texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "text_embeddings = model.get_text_features(**text_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(images=images, return_tensors=\"pt\", padding=True)\n",
    "image_embeddings = model.get_image_features(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salva os embeddings e listas nos arquivos JSON na pasta \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(text_embeddings, 'data/text_embeddings.pt')\n",
    "save(image_embeddings, 'data/image_embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(data, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "save_to_json(texts, 'data/text_content.json')\n",
    "save_to_json(images_list, 'data/image_content.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
