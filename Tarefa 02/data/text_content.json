[
    {
        "article_title": "Multimodal Models ‚Äî LLMs that can see and hear",
        "sections": [
            {
                "Multimodal Models ‚Äî LLMs That Can See and Hear": "An introduction with example Python¬†code\n\nThis is the first post in a larger series on Multimodal AI. A Multimodal Model (MM) is an AI system capable of processing or generating multiple data modalities (e.g., text, image, audio, video). In this article, I will discuss a particular type of MM that builds on top of a large language model (LLM). I‚Äôll start with a high-level overview of such models and then share example code for using LLaMA 3.2 Vision to perform various image-to-text tasks.\n\nPhoto by Sincerely Media on¬†Unsplash\n\n"
            },
            {
                "What is a Multimodal Model?": "A Multimodal Model (MM) is an AI system that can process multiple data modalities as input or output (or both) [1]. Below are a few examples.\n\nGPT-4o‚Ää‚Äî‚ÄäInput: text, images, and audio. Output: text.FLUX‚Ää‚Äî‚ÄäInput: text. Output: images.Suno‚Ää‚Äî‚ÄäInput: text. Output: audio.\n\nExample mutlimodal models. Image by¬†author.\n\nWhile there are several ways to create models that can process multiple data modalities, a recent line of research seeks to use LLMs as the core reasoning engine of a multimodal system [2]. Such models are called multimodal large language models (or large multimodal models) [2][3].\n\nOne benefit of using existing LLM as a starting point for MMs is that they‚Äôve demonstrated a strong ability to acquire world knowledge through large-scale pre-training, which can be leveraged to process concepts appearing in non-textual representations.\n\n"
            },
            {
                "3 Paths to Multimodality": "Here, I will focus on multimodal models developed from an LLM. Three popular approaches are described below.\n\nLLM + Tools: Augment LLMs with pre-built componentsLLM + Adapters: Augment LLMs with multi-modal encoders or decoders, which are aligned via adapter fine-tuningUnified Models: Expand LLM architecture to fuse modalities at pre-training\n\n"
            },
            {
                "Path 1: LLM + Tools": "The simplest way to make an LLM multimodal is by adding external modules that can readily translate between text and an arbitrary modality. For example, a transcription model (e.g. Whisper) can be connected to an LLM to translate input speech into text, or a text-to-image model can generate images based on LLM outputs.\n\nThe key benefit of such an approach is simplicity. Tools can quickly be assembled without any additional model training.\n\nThe downside, however, is that the quality of such a system may be limited. Just like when playing a game of telephone, messages mutate when passed from person to person. Information may degrade going from one module to another via text descriptions only.\n\nAn example of information degradation during message passing. Image by¬†author.\n\n"
            },
            {
                "Path 2: LLM + Adapters": "One way to mitigate the ‚Äútelephone problem‚Äù is by optimizing the representations of new modalities to align with the LLM‚Äôs internal concept space. For example, ensuring an image of a dog and the description of one look similar to the LLM.\n\nThis is possible through the use of adapters, a relatively small set of parameters that appropriately translate a dense vector representation for a downstream model [2][4][5].\n\nAdapters can be trained using, for example, image-caption pairs, where the adapter learns to translate an image encoding into a representation compatible with the LLM [2][4][6]. One way to achieve this is via contrastive learning [2], which I will discuss more in the next article of this series.\n\nA simple strategy for integrating images into an LLM via an image encoding adapter. Image by¬†author.\n\nThe benefits of using adapters to augment LLMs include better alignment between novel modality representations in a data-efficient way. Since many pre-trained embedding, language, and diffusion models are available in today‚Äôs AI landscape, one can readily fuse models based on their needs. Notable examples from the open-source community are LLaVA, LLaMA 3.2 Vision, Flamingo, MiniGPT4, Janus, Mini-Omni2, and IDEFICS [3][5][7][8].\n\nHowever, this data efficiency comes at a price. Just like how adapter-based fine-tuning approaches (e.g. LoRA) can only nudge an LLM so far, the same holds in this context. Additionally, pasting various encoders and decoders to an LLM may result in overly complicated model architectures.\n\n"
            },
            {
                "Path 3: Unified Models": "The final way to make an LLM multimodal is by incorporating multiple modalities at the pre-training stage. This works by adding modality-specific tokenizers (rather than pre-trained encoder/decoder models) to the model architecture and expanding the embedding layer to accommodate new modalities [9].\n\nWhile this approach comes with significantly greater technical challenges and computational requirements, it enables the seamless integration of multiple modalities into a shared concept space, unlocking better reasoning capabilities and efficiencies [10].\n\nThe preeminent example of this unified approach is (presumably) GPT-4o, which processes text, image, and audio inputs to enable expanded reasoning capabilities at faster inference times than previous versions of GPT-4. Other models that follow this approach include Gemini, Emu3, BLIP, and Chameleon [9][10].\n\nTraining these models typically entails multi-step pre-training on a set of (multimodal) tasks, such as language modeling, text-image contrastive learning, text-to-video generation, and others [7][9][10].\n\n"
            },
            {
                "Example: Using LLaMA 3.2 Vision for Image-based Tasks": "With a basic understanding of how LLM-based multimodal models work under the hood, let‚Äôs see what we can do with them. Here, I will use LLaMA 3.2 Vision to perform various image-to-text tasks.\n\nTo run this example, download Ollama and its Python library. This enables the model to run locally i.e. no need for external API calls.\n\nThe example code is freely available on GitHub.\n\n"
            },
            {
                "What‚Äôs next?": "Multimodal models are AI systems that can process multiple data modalities as inputs or outputs (or both). A recent trend for developing these systems involves adding modalities to large language models (LLMs).\n\nHowever, there are other types of multimodal models. In the next article of this series, I will discuss multimodal embedding models, which encode multiple data modalities (e.g. text and images) into a shared representation space.\n\nMore on Multimodal models üëá\n\nMultimodal AIshawhin.medium.com\n\n"
            }
        ]
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "sections": [
            {
                "Multimodal Embeddings: An Introduction": "Mapping text and images into a common¬†space\n\nThis is the 2nd article in a larger series on multimodal AI. In the previous post, we saw how to augment large language models (LLMs) to understand new data modalities (e.g., images, audio, video). One such approach relied on encoders that generate vector representations (i.e. embeddings) of non-text data. In this article, I will discuss multimodal embeddings and share what they can do via two practical use cases.\n\nImage from¬†Canva.\n\n"
            },
            {
                "Embeddings": "Embeddings are (useful) numerical representations of data learned implicitly through model training. For example, through learning how to predict text, BERT learned representations of text, which are helpful for many NLP tasks [1]. Another example is the Vision Transformer (ViT), trained for image classification on Image Net, which can be repurposed for other applications [2].\n\nA key point here is that these learned embedding spaces will have some underlying structure so that similar concepts are located close together. As shown in the toy examples below.\n\nToy represetation of text and image embeddings, respectively. Image by¬†author.\n\nOne key limitation of the previously mentioned models is they are restricted to a single data modality, e.g., text or images. Preventing cross-modal applications like image captioning, content moderation, image search, and more. But what if we could merge these two representations?\n\n"
            },
            {
                "Multimodal Embeddings": "Although text and images may look very different to us, in a neural network, these are represented via the same mathematical object, i.e., a vector. Therefore, in principle, text, images, or any other data modality can processed by a single model.\n\nThis fact underlies multimodal embeddings, which represent multiple data modalities in the same vector space such that similar concepts are co-located (independent of their original representations).\n\nToy representation of multimodal embedding space. Image by¬†author.\n\nFor example, CLIP encodes text and images into a shared embedding space [3]. A key insight from CLIP is that by aligning text and image representations, the model is capable of 0-shot image classification on an arbitrary set of target classes since any input text can be treated as a class label (we will see a concrete example of this later).\n\nHowever, this idea is not limited to text and images. Virtually any data modalities can be aligned in this way e.g., text-audio, audio-image, text-EEG, image-tabular, and text-video. Unlocking use cases such as video captioning, advanced OCR, audio transcription, video search, and EEG-to-text [4].\n\n"
            },
            {
                "Contrastive Learning": "The standard approach to aligning disparate embedding spaces is contrastive learning (CL). A key intuition of CL is to represent different views of the same information similarly [5].\n\nThis consists of learning representations that maximize the similarity between positive pairs and minimize the similarity of negative pairs. In the case of an image-text model, a positive pair might be an image with an appropriate caption, while a negative pair would be an image with an irrelevant caption (as shown below).\n\nExample positive and negative pairs used in contrastive training. Image by¬†author.\n\nTwo key aspects of CL contribute to its effectiveness\n\nSince positive and negative pairs can be curated from the data‚Äôs inherent structure (e.g., metadata from web images), CL training data do not require manual labeling, which unlocks larger-scale training and more powerful representations [3].It simultaneously maximizes positive and minimizes negative pair similarity via a special loss function, as demonstrated by CLIP [3].\n\nCLIP‚Äôs contrastive loss for text-image representation alignment [3]. Image by¬†author.\n\n"
            },
            {
                "Example Code: Using CLIP for 0-shot classification and image search": "With a high-level understanding of how multimodal embeddings work, let‚Äôs see two concrete examples of what they can do. Here, I will use the open-source CLIP model to perform two tasks: 0-shot image classification and image search.\n\nThe code for these examples is freely available on the GitHub repository.\n\n"
            },
            {
                "What‚Äôs Next?": "Multimodal embeddings unlock countless AI use cases that involve multiple data modalities. Here, we saw two such use cases, i.e., 0-shot image classification and image search using CLIP.\n\nAnother practical application of models like CLIP is multimodal RAG, which consists of the automated retrieval of multimodal context to an LLM. In the next article of this series, we will see how this works under the hood and review a concrete example.\n\nMore on Multimodal models üëá\n\nMultimodal AIEdit descriptionshawhin.medium.com\n\n"
            }
        ]
    }
]